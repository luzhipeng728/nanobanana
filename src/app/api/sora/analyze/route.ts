import { NextRequest } from "next/server";
import Anthropic from "@anthropic-ai/sdk";
import { CLAUDE_LIGHT_MODEL, CLAUDE_LIGHT_MAX_TOKENS } from "@/lib/claude-config";

export interface SoraAnalyzeEvent {
  type: "status" | "analysis_start" | "analysis_chunk" | "analysis_end" | "prompt_ready" | "error";
  step?: string;
  progress?: number;
  chunk?: string;
  prompt?: string;
  analysis?: string;
  error?: string;
}

/**
 * Sora è§†é¢‘æ™ºèƒ½åˆ†æ API
 * 1. åˆ†æè¾“å…¥å›¾ç‰‡çš„å†…å®¹ã€äººç‰©ã€åœºæ™¯
 * 2. æ ¹æ®æ—¶é•¿ç”Ÿæˆè¯¦ç»†çš„è§†é¢‘æè¿°ï¼ˆåŒ…å«å¿ƒç†æ´»åŠ¨ã€è¡¨æƒ…å˜åŒ–ã€åŠ¨ä½œç»†èŠ‚ï¼‰
 */
export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();

  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: SoraAnalyzeEvent) => {
    await writer.write(encoder.encode(`data: ${JSON.stringify(event)}\n\n`));
  };

  (async () => {
    try {
      const body = await request.json();
      const { userRequest, imageUrl, durationSeconds = 8 } = body as {
        userRequest: string;
        imageUrl?: string;
        durationSeconds?: number;
      };

      if (!userRequest) {
        await sendEvent({ type: "error", error: "è¯·è¾“å…¥è§†é¢‘æè¿°" });
        await writer.close();
        return;
      }

      const apiKey = process.env.ANTHROPIC_API_KEY;
      if (!apiKey) {
        await sendEvent({ type: "error", error: "ANTHROPIC_API_KEY æœªé…ç½®" });
        await writer.close();
        return;
      }

      const anthropic = new Anthropic({
        apiKey,
        baseURL: process.env.ANTHROPIC_BASE_URL || undefined,
      });

      let imageAnalysis = "";

      // ç¬¬ä¸€é˜¶æ®µï¼šè¯¦ç»†åˆ†æå›¾ç‰‡
      if (imageUrl) {
        await sendEvent({
          type: "status",
          step: "ğŸ‘ï¸ AI æ­£åœ¨åˆ†æå›¾ç‰‡å†…å®¹...",
          progress: 10,
        });

        await sendEvent({ type: "analysis_start" });

        // æ„å»ºå›¾ç‰‡å†…å®¹
        const imageContent: Anthropic.ImageBlockParam[] = [];
        if (imageUrl.startsWith("data:")) {
          const match = imageUrl.match(/^data:([^;]+);base64,(.+)$/);
          if (match) {
            imageContent.push({
              type: "image",
              source: {
                type: "base64",
                media_type: match[1] as "image/jpeg" | "image/png" | "image/gif" | "image/webp",
                data: match[2],
              },
            });
          }
        } else {
          imageContent.push({
            type: "image",
            source: {
              type: "url",
              url: imageUrl,
            },
          });
        }

        // æµå¼åˆ†æå›¾ç‰‡ - æ”¯æŒåˆ†é•œå›¾è¯†åˆ«ï¼ˆåŸºäº Sora 2 æœ€ä½³å®è·µï¼‰
        const analysisStream = anthropic.messages.stream({
          model: CLAUDE_LIGHT_MODEL,
          max_tokens: CLAUDE_LIGHT_MAX_TOKENS,
          messages: [
            {
              role: "user",
              content: [
                ...imageContent,
                {
                  type: "text",
                  text: `ä½œä¸ºä¸“ä¸šè§†é¢‘å¯¼æ¼”ï¼Œè¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œä¸º Sora 2 è§†é¢‘ç”Ÿæˆåšå‡†å¤‡ã€‚

ç”¨æˆ·çš„åˆ›æ„æƒ³æ³•ï¼š${userRequest}
è§†é¢‘æ—¶é•¿ï¼š${durationSeconds} ç§’

**é¦–å…ˆåˆ¤æ–­ï¼šè¿™æ˜¯åˆ†é•œå›¾ï¼ˆStoryboardï¼Œå¤šä¸ªç”»é¢/é¢æ¿ï¼‰è¿˜æ˜¯å•å¼ å›¾ç‰‡ï¼Ÿ**

---

## å¦‚æœæ˜¯ã€åˆ†é•œå›¾/æ•…äº‹æ¿ã€‘ï¼ˆå¤šä¸ªç”»é¢/é¢æ¿ï¼‰ï¼š

âš ï¸ **é‡è¦**ï¼šè¿™æ˜¯ç”¨äºæŒ‡å¯¼"çœŸå®è§†é¢‘æ‹æ‘„"çš„æ•…äº‹æ¿ï¼Œä¸æ˜¯è¦å¤åˆ¶çš„æ¼«ç”»ï¼
- åˆ†é•œå›¾æ˜¯"æ‹æ‘„å‚è€ƒ"ï¼Œæœ€ç»ˆè§†é¢‘åº”è¯¥æ˜¯**å†™å®é£æ ¼**æˆ–**ç”µå½±åŠ¨ç”»é£æ ¼**
- ä¸è¦ç”Ÿæˆæ¼«ç”»é£æ ¼ã€ä¸è¦ç”Ÿæˆå›¾ç‰‡åˆ‡æ¢ã€ä¸è¦ä¿ç•™åˆ†é•œçš„çº¿æ¡/æ ¼å­
- è¦ç”ŸæˆçœŸå®çš„äººç‰©åŠ¨ä½œã€çœŸå®çš„åœºæ™¯ã€æµç•…çš„è¿åŠ¨

æŒ‰ç…§åˆ†é•œé¡ºåºï¼Œç”¨ç”µå½±é•œå¤´è¯­è¨€æè¿°**çœŸå®æ‹æ‘„**æ•ˆæœï¼š

### Panel 1 (00:00-XX:XX)
- **çœŸå®åœºæ™¯**ï¼šå°†æ¼«ç”»åœºæ™¯è½¬åŒ–ä¸ºçœŸå®ç¯å¢ƒæè¿°
- **äººç‰©å¤–è§‚**ï¼šåŸºäºåˆ†é•œæè¿°çœŸå®äººç‰©ï¼ˆä¸æ˜¯æ¼«ç”»è§’è‰²ï¼‰
- **çœŸå®åŠ¨ä½œ**ï¼šäººç‰©å¦‚ä½•çœŸå®åœ°ç§»åŠ¨ã€è¯´è¯ã€è¡¨æƒ…å˜åŒ–
- **æ‘„å½±æœº**ï¼šé•œå¤´ç±»å‹ã€è§’åº¦ã€è¿åŠ¨
- **å¯¹ç™½å¤„ç†**ï¼šå¦‚æœåˆ†é•œæœ‰æ–‡å­—å¯¹ç™½ï¼Œæè¿°äººç‰©"è¯´è¯"çš„åŠ¨ä½œ

### Panel 2 (XX:XX-XX:XX)
...ä»¥æ­¤ç±»æ¨

**å…³é”®è½¬åŒ–åŸåˆ™**ï¼š
- æ¼«ç”»åˆ†é•œ â†’ çœŸå®è§†é¢‘é•œå¤´
- æ¼«ç”»å¯¹ç™½æ¡† â†’ äººç‰©è¯´è¯åŠ¨ä½œï¼ˆå˜´å”‡åŠ¨ã€è¡¨æƒ…é…åˆï¼‰
- æ¼«ç”»ç‰¹æ•ˆçº¿ â†’ çœŸå®ç‰©ç†è¿åŠ¨ï¼ˆé€Ÿåº¦æ„Ÿã€æ¨¡ç³Šæ•ˆæœï¼‰
- æ¼«ç”»é£æ ¼ â†’ å†™å®/ç”µå½±é£æ ¼

**æ—¶é—´åˆ†é…**ï¼šæ ¹æ® ${durationSeconds} ç§’æ€»æ—¶é•¿ï¼Œåˆç†åˆ†é…æ¯ä¸ªé•œå¤´çš„æ—¶é—´æˆ³ã€‚

---

## å¦‚æœæ˜¯ã€å•å¼ å›¾ç‰‡ã€‘ï¼š

### 1. ä¸»ä½“åˆ†æ
- å¤–è²Œç‰¹å¾ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€å‘å‹ã€æœè£…ç»†èŠ‚ï¼‰
- å½“å‰å§¿æ€å’Œè¡¨æƒ…
- æ€§æ ¼æš—ç¤ºå’Œæƒ…ç»ªçŠ¶æ€

### 2. åœºæ™¯ç¯å¢ƒ
- åœ°ç‚¹å’Œæ—¶é—´ï¼ˆå®¤å†…/å®¤å¤–ã€ç™½å¤©/å¤œæ™šï¼‰
- ç©ºé—´å¸ƒå±€ï¼ˆå‰æ™¯ã€ä¸­æ™¯ã€èƒŒæ™¯å…ƒç´ ï¼‰
- å…‰çº¿æ¡ä»¶ï¼ˆå…‰æºæ–¹å‘ã€å¼ºåº¦ã€è‰²æ¸©ï¼‰

### 3. åŠ¨æ€è§„åˆ’ï¼ˆ${durationSeconds}ç§’æ—¶é—´çº¿ï¼‰
æ ¹æ®æ—¶é•¿è§„åˆ’åŠ¨ä½œåºåˆ—ï¼š
- **å¼€åœº (0-2ç§’)**ï¼šå»ºç«‹åˆå§‹çŠ¶æ€
- **å‘å±• (ä¸­æ®µ)**ï¼šä¸»è¦åŠ¨ä½œ/æƒ…ç»ªå˜åŒ–
- **æ”¶å°¾ (æœ€å1-2ç§’)**ï¼šè‡ªç„¶ç»“æŸçŠ¶æ€

å…·ä½“åŠ¨æ€å…ƒç´ ï¼š
- äººç‰©åŠ¨ä½œï¼ˆå¾®è¡¨æƒ…ã€çœ¨çœ¼ã€å‘¼å¸ã€è½¬å¤´ã€æ‰‹åŠ¿ï¼‰
- ç¯å¢ƒåŠ¨æ€ï¼ˆé£å¹å¤´å‘/è¡£æœã€å…‰å½±å˜åŒ–ã€èƒŒæ™¯å…ƒç´ è¿åŠ¨ï¼‰

### 4. æ‘„å½±æœºå»ºè®®
- æ¨èé•œå¤´ç±»å‹å’Œç„¦è·
- æ˜¯å¦éœ€è¦é•œå¤´è¿åŠ¨
- æ™¯æ·±å»ºè®®ï¼ˆæµ…æ™¯æ·±èšç„¦ä¸»ä½“ vs æ·±æ™¯æ·±å±•ç¤ºç¯å¢ƒï¼‰

### 5. æƒ…æ„Ÿ/å¿ƒç†
- å†…å¿ƒç‹¬ç™½ï¼ˆtaåœ¨æƒ³ä»€ä¹ˆï¼Ÿï¼‰
- æƒ…ç»ªå¼§çº¿ï¼ˆä»ä»€ä¹ˆæƒ…ç»ªåˆ°ä»€ä¹ˆæƒ…ç»ªï¼‰

---

è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°ï¼Œåƒç»™æ‘„å½±å¸ˆåš briefing ä¸€æ ·å…·ä½“ã€‚`,
                },
              ],
            },
          ],
        });

        for await (const event of analysisStream) {
          if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
            const chunk = event.delta.text;
            imageAnalysis += chunk;
            await sendEvent({ type: "analysis_chunk", chunk });
          }
        }

        await sendEvent({ type: "analysis_end" });

        await sendEvent({
          type: "status",
          step: "âœ¨ æ­£åœ¨åˆ›ä½œè§†é¢‘å‰§æœ¬...",
          progress: 60,
        });
      } else {
        // æ— å›¾ç‰‡æ¨¡å¼ï¼šå…ˆåˆ†æç”¨æˆ·éœ€æ±‚ï¼Œæµå¼å±•ç¤ºæ€è€ƒè¿‡ç¨‹
        await sendEvent({
          type: "status",
          step: "ğŸ§  AI æ­£åœ¨ç†è§£ä½ çš„åˆ›æ„...",
          progress: 10,
        });

        await sendEvent({ type: "analysis_start" });

        // æµå¼åˆ†æç”¨æˆ·éœ€æ±‚ï¼ˆåŸºäº Sora 2 æœ€ä½³å®è·µï¼‰
        const thinkingStream = anthropic.messages.stream({
          model: CLAUDE_LIGHT_MODEL,
          max_tokens: CLAUDE_LIGHT_MAX_TOKENS,
          messages: [
            {
              role: "user",
              content: `ä½œä¸ºä¸“ä¸šç”µå½±æ‘„å½±å¸ˆï¼Œè¯·ä¸º Sora 2 è§†é¢‘ç”Ÿæˆåˆ†æç”¨æˆ·çš„åˆ›æ„éœ€æ±‚ã€‚

ç”¨æˆ·çš„åˆ›æ„æƒ³æ³•ï¼š${userRequest}
è§†é¢‘æ—¶é•¿ï¼š${durationSeconds} ç§’

è¯·åƒç»™æ‘„å½±å›¢é˜Ÿåš briefing ä¸€æ ·è¯¦ç»†åˆ†æï¼š

### 1. åˆ›æ„è§£è¯»
- ç”¨æˆ·æƒ³è¦è¡¨è¾¾çš„æ ¸å¿ƒä¸»é¢˜/æƒ…æ„Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
- ç”»é¢çš„ "é’©å­" æ˜¯ä»€ä¹ˆï¼ˆæœ€å¸å¼•äººçš„è§†è§‰å…ƒç´ ï¼‰ï¼Ÿ

### 2. åœºæ™¯è®¾è®¡
- **åœ°ç‚¹**ï¼šå…·ä½“åœºæ™¯ï¼ˆå®¤å†…/å®¤å¤–/åŸå¸‚/è‡ªç„¶ç­‰ï¼‰
- **æ—¶é—´**ï¼šç™½å¤©/å¤œæ™š/é»„é‡‘æ—¶æ®µ/è“è°ƒæ—¶åˆ»
- **æ°›å›´**ï¼šè‰²è°ƒï¼ˆæš–/å†·ï¼‰ã€å¤©æ°”ã€ç¯å¢ƒç»†èŠ‚
- **å…‰çº¿**ï¼šä¸»å…‰æºæ–¹å‘ã€å¼ºåº¦ã€è‰²æ¸©

### 3. ä¸»ä½“è®¾è®¡
- äººç‰©/ç‰©ä½“çš„è¯¦ç»†å¤–è§‚æè¿°
- æœè£…ã€æè´¨ã€é¢œè‰²
- åˆå§‹å§¿æ€å’Œè¡¨æƒ…

### 4. åŠ¨ä½œæ—¶é—´çº¿ï¼ˆ${durationSeconds}ç§’ï¼‰
${durationSeconds <= 4
  ? "çŸ­ç‰‡èŠ‚å¥ï¼šèšç„¦å•ä¸€åŠ¨ä½œæˆ–æƒ…ç»ªç¬é—´\n- 0-1ç§’ï¼šå»ºç«‹\n- 1-3ç§’ï¼šæ ¸å¿ƒåŠ¨ä½œ\n- 3-4ç§’ï¼šå®šæ ¼"
  : durationSeconds <= 8
    ? "ä¸­ç­‰èŠ‚å¥ï¼šå¯åŒ…å«æƒ…ç»ªå¼§çº¿\n- 0-2ç§’ï¼šå»ºç«‹åœºæ™¯å’Œåˆå§‹çŠ¶æ€\n- 2-6ç§’ï¼šä¸»è¦åŠ¨ä½œ/æƒ…ç»ªå‘å±•\n- 6-8ç§’ï¼šæ”¶å°¾/æƒ…ç»ªé‡Šæ”¾"
    : "å™äº‹èŠ‚å¥ï¼šå¯æ„å»ºå®Œæ•´æ•…äº‹\n- 0-3ç§’ï¼šåœºæ™¯å»ºç«‹\n- 3-8ç§’ï¼šåŠ¨ä½œå‘å±•å’Œæƒ…ç»ªå˜åŒ–\n- 8-12ç§’ï¼šé«˜æ½®å’Œç»“å°¾"}

### 5. æ‘„å½±æœºè®¾è®¡
- **é•œå¤´ç±»å‹**ï¼šwide/medium/close-up
- **ç„¦è·å»ºè®®**ï¼š24mmå¹¿è§’/50mmæ ‡å‡†/85mmäººåƒ
- **è§’åº¦**ï¼šå¹³è§†/ä»°æ‹/ä¿¯æ‹
- **è¿åŠ¨**ï¼šé™æ­¢/æ¨è¿›/è·Ÿè¸ª/æ‘‡ç§»
- **æ™¯æ·±**ï¼šæµ…æ™¯æ·±ï¼ˆçªå‡ºä¸»ä½“ï¼‰/ æ·±æ™¯æ·±ï¼ˆç¯å¢ƒå™äº‹ï¼‰

### 6. ç‰©ç†ç»†èŠ‚
- å¤´å‘/è¡£ç‰©å¦‚ä½•éšé£æˆ–åŠ¨ä½œç§»åŠ¨
- å…‰çº¿å¦‚ä½•ä¸æè´¨äº¤äº’ï¼ˆåå°„ã€é€å°„ï¼‰
- ç¯å¢ƒç²’å­ï¼ˆç°å°˜ã€é›¨æ»´ã€èŠ±ç“£ç­‰ï¼‰

è¯·ç”¨ä¸­æ–‡è¯¦ç»†åˆ†æï¼Œè¿™äº›ä¿¡æ¯å°†ç”¨äºç”Ÿæˆä¸“ä¸šçš„ Sora 2 æç¤ºè¯ã€‚`,
            },
          ],
        });

        for await (const event of thinkingStream) {
          if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
            const chunk = event.delta.text;
            imageAnalysis += chunk;
            await sendEvent({ type: "analysis_chunk", chunk });
          }
        }

        await sendEvent({ type: "analysis_end" });

        await sendEvent({
          type: "status",
          step: "âœ¨ æ­£åœ¨ç”Ÿæˆä¸“ä¸šæç¤ºè¯...",
          progress: 60,
        });
      }

      // ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆ Sora è§†é¢‘æç¤ºè¯
      const durationGuide = getDurationGuide(durationSeconds);

      // æ£€æµ‹æ˜¯å¦æ˜¯åˆ†é•œåˆ†æ
      const isStoryboard = imageAnalysis.includes("åˆ†é•œ") || imageAnalysis.includes("Storyboard") || imageAnalysis.includes("é¢æ¿");

      const promptSystemMessage = imageUrl
        ? `You are an expert cinematographer crafting prompts for OpenAI Sora 2.

## Your Analysis:
${imageAnalysis}

## User's Creative Idea: ${userRequest}
## Video Duration: ${durationSeconds} seconds
## Mode: ${isStoryboard ? "STORYBOARD (Multi-Shot Sequence)" : "Single Image Animation"}

${durationGuide}

${isStoryboard ? `
## STORYBOARD MODE - REALISTIC VIDEO (NOT COMIC SLIDESHOW!)

âš ï¸ CRITICAL: The storyboard is a REFERENCE for filming, NOT content to reproduce!
- Output must be PHOTOREALISTIC or CINEMATIC ANIMATION style
- NO comic/manga aesthetics, NO panel borders, NO speech bubbles
- Generate REAL human movements, REAL environments, FLUID motion
- If storyboard has dialogue text, describe characters SPEAKING (lips moving, expressions)

**Transform each panel into a REAL video shot:**

**Required Elements:**
- Setting: Real-world environment (not comic background)
- Characters: Photorealistic humans with detailed features (not cartoon)
- Motion: Continuous fluid movement (not static poses)
- Camera: Professional cinematography (lens, angle, movement)
- Physics: Real hair movement, fabric motion, environmental effects
- Dialogue: If text in storyboard, show character speaking naturally

**Example (10s storyboard â†’ realistic video):**
"Cinematic wide shot of a rain-soaked city street at dusk, neon signs reflecting on wet asphalt (00:00-00:02). A young woman in a red trench coat walks purposefully toward camera, her heels splashing in puddles, hair damp from rain (00:02-00:05). Medium close-up: she pauses, water droplets on her face catching the neon glow, her eyes searching left then right with subtle concern (00:05-00:08). Over-shoulder shot: her hand pushes open a heavy wooden door, warm amber light spilling out onto her face as she steps inside (00:08-00:10)."

Generate a CINEMATIC video prompt (150-220 words) that transforms the storyboard into REALISTIC footage.
` : `
## SINGLE IMAGE MODE - Cinematic Animation

Transform this static image into a living moment. Structure your prompt like a director's brief:

**Layer 1 - Setting & Atmosphere:**
Environment, time of day, weather, lighting direction and color temperature.

**Layer 2 - Subject & State:**
Detailed appearance, current pose, expression, implied emotion.

**Layer 3 - Motion Timeline (${durationSeconds}s):**
- Opening (0-2s): Initial state, camera establishes scene
- Development: Primary action/emotion shift
- Resolution: Natural ending state

**Layer 4 - Micro-Details:**
Breathing, blinking, hair movement, fabric motion, environmental particles.

**Layer 5 - Camera:**
Shot type, focal length (35mm/50mm/85mm), depth of field, any movement.

**Example (8s single image):**
"Medium close-up, 50mm lens, shallow depth of field. A young woman with auburn hair sits by a rain-streaked window, soft diffused daylight from the left casting gentle shadows. She gazes outward, lost in thought. Slowly, she turns her head toward camera, a faint smile forming as if remembering something precious. Her fingers trace the condensation on the glass. Wind from an open window stirs loose strands of her hair. In the background, blurred cherry blossoms drift past the glass like scattered memories."

Generate a rich cinematic prompt (100-180 words).
`}

## SORA 2 PROMPT RULES:
- Style: PHOTOREALISTIC or CINEMATIC (never comic/manga/cartoon unless explicitly requested)
- Write in present tense (what IS happening)
- Be specific about camera: lens (35mm/50mm/85mm), angle, movement
- Include physics: wind, material behavior, light interaction, reflections
- Add micro-movements: breathing, blinking, hair sway, fabric motion
- Describe spatial relationships: foreground, background, depth
- For dialogue scenes: describe lips moving, facial expressions matching speech
- Keep one continuous shot unless storyboard mode requires cuts

**NEVER include in output:**
- Comic/manga art style descriptions
- Panel borders or speech bubbles
- Static pose descriptions
- "Illustration" or "drawing" or "artwork" language

Output ONLY the English prompt text, no explanations.`
        : `You are an expert cinematographer crafting prompts for OpenAI Sora 2.

## Your Creative Analysis:
${imageAnalysis}

## User's Original Idea: ${userRequest}
## Video Duration: ${durationSeconds} seconds

${durationGuide}

## Your Task:
Transform this concept into a professional Sora 2 video prompt using cinematography language.

**Prompt Structure:**
1. **Setting**: Location, time, atmosphere, lighting (direction, color, intensity)
2. **Subject**: Who/what, detailed appearance, current state
3. **Action**: Specific visible movements over ${durationSeconds} seconds
4. **Camera**: Shot type, focal length, angle, movement (if any)
5. **Physics**: Material behavior, wind, particles, reflections
6. **Mood**: Emotional undertone, psychological depth

**Example:**
"Wide shot, 35mm lens, golden hour. A weathered craftsman with silver-streaked hair stands in his workshop, amber light streaming through dusty windows. He carefully shapes glowing metal on an anvil, sparks dancing upward like fireflies. His calloused hands move with practiced precision. Sweat beads on his forehead. The warm forge-glow illuminates floating dust particles. He pauses, examines his work, then nods with quiet satisfaction."

Generate a vivid, cinematic prompt (100-180 words) with specific camera and physics details.
Output ONLY the English prompt text, no explanations.`;

      const promptResponse = await anthropic.messages.create({
        model: CLAUDE_LIGHT_MODEL,
        max_tokens: CLAUDE_LIGHT_MAX_TOKENS,
        messages: [{ role: "user", content: promptSystemMessage }],
      });

      const textBlock = promptResponse.content.find(
        (block): block is Anthropic.TextBlock => block.type === "text"
      );

      const generatedPrompt = textBlock?.text?.trim() || userRequest;

      await sendEvent({
        type: "status",
        step: "âœ… è§†é¢‘å‰§æœ¬åˆ›ä½œå®Œæˆï¼",
        progress: 100,
      });

      await sendEvent({
        type: "prompt_ready",
        prompt: generatedPrompt,
        analysis: imageAnalysis || undefined,
      });
    } catch (error) {
      console.error("Sora analyze error:", error);
      await sendEvent({
        type: "error",
        error: error instanceof Error ? error.message : "åˆ†æå¤±è´¥",
      });
    } finally {
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

/**
 * æ ¹æ®æ—¶é•¿æä¾›ä¸“ä¸šçš„ Sora 2 åˆ›ä½œæŒ‡å¯¼
 */
function getDurationGuide(seconds: number): string {
  if (seconds <= 4) {
    return `## Duration: ${seconds}s (Single Moment)

**Pacing Strategy:**
- One camera setup, one focused action
- Ideal for: a glance, a gesture, a reaction
- Keep it simple but visually striking

**Shot Structure:**
- 0-1s: Establish subject and setting
- 1-3s: Single key action or emotion
- 3-4s: Hold or subtle resolve

**Camera:** Static or minimal movement (slight push-in works well)
**Avoid:** Multiple cuts, complex sequences, too many actions`;
  } else if (seconds <= 8) {
    return `## Duration: ${seconds}s (Emotional Arc)

**Pacing Strategy:**
- Room for one emotional transition
- Can include: setup â†’ development â†’ micro-resolution
- Best for character moments and atmospheric pieces

**Shot Structure:**
- 0-2s: Establish scene, subject in initial state
- 2-5s: Primary action unfolds, emotion shifts
- 5-8s: Resolution, new emotional state, or lingering moment

**Camera:** Can include one smooth movement (dolly, slow pan)
**Physics:** Add environmental motion - wind, light shifts, particles
**Micro-details:** Breathing, blinking, hair movement, fabric shifts`;
  } else {
    return `## Duration: ${seconds}s (Mini Narrative)

**Pacing Strategy:**
- Full story arc possible: beginning â†’ middle â†’ end
- Can include 2-3 distinct beats or shots
- Best for short scenes with clear progression

**Shot Structure:**
- 0-3s: Wide establishing, set the scene
- 3-7s: Action develops, emotional journey
- 7-10s: Climax or key moment
- 10-12s: Resolution, final state

**Camera:** Multiple angles possible, use cuts or continuous movement
**Storytelling:** Include cause-and-effect, character motivation
**Physics:** Complex interactions (objects, environment, lighting changes)
**Audio consideration:** If using audio, time dialogue to key moments`;
  }
}
