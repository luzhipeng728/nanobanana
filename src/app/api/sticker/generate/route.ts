import { NextRequest } from "next/server";
import Anthropic from "@anthropic-ai/sdk";
import { v4 as uuidv4 } from "uuid";
import { prisma } from "@/lib/prisma";
import { generateImageAction } from "@/app/actions/generate";
import type { GeminiImageModel, ImageGenerationConfig } from "@/types/image-gen";

// åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯
function getAnthropicClient() {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) {
    throw new Error("ANTHROPIC_API_KEY æœªé…ç½®");
  }
  return new Anthropic({
    apiKey,
    baseURL: process.env.ANTHROPIC_BASE_URL || undefined,
  });
}

// åˆ†æåŸå§‹å›¾ç‰‡ï¼Œè·å–åŸºç¡€æè¿°ï¼ˆæµå¼ï¼‰
async function analyzeOriginalImage(
  imageUrl: string,
  animationPrompt: string,
  onChunk: (chunk: string) => Promise<void>
): Promise<string> {
  const anthropic = getAnthropicClient();
  let analysisText = "";
  
  const stream = anthropic.messages.stream({
    model: "claude-sonnet-4-5-20250929",
    max_tokens: 1500,
    messages: [
      {
        role: "user",
        content: [
          {
            type: "image",
            source: { type: "url", url: imageUrl },
          },
          {
            type: "text",
            text: `è¯·ä»”ç»†åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œæˆ‘éœ€è¦åŸºäºå®ƒç”Ÿæˆä¸€ä¸ªã€Œ${animationPrompt}ã€åŠ¨ç”»æ•ˆæœã€‚

è¯·ç”¨è‹±æ–‡æè¿°ä»¥ä¸‹å†…å®¹ï¼ˆè¿™å°†ç”¨äºå›¾åƒç”Ÿæˆï¼‰ï¼š
1. **Subject**: ä¸»ä½“çš„å¤–å½¢ã€é¢œè‰²ã€å§¿æ€ã€è¡¨æƒ…ç­‰è¯¦ç»†ç‰¹å¾
2. **Background**: èƒŒæ™¯çš„é¢œè‰²ã€å…ƒç´ ã€æ°›å›´
3. **Art Style**: ç”»é£ã€è‰²è°ƒã€è´¨æ„Ÿ
4. **Animation Plan**: å¦‚ä½•å°†"${animationPrompt}"è¿™ä¸ªåŠ¨ç”»åˆ†è§£ä¸º10å¸§çš„å¾®å°æ¸è¿›å˜åŒ–

è¯·ç›´æ¥ç”¨è‹±æ–‡è¾“å‡ºï¼Œæ ¼å¼æ¸…æ™°ã€‚`,
          },
        ],
      },
    ],
  });

  for await (const event of stream) {
    if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
      const chunk = event.delta.text;
      analysisText += chunk;
      await onChunk(chunk);
    }
  }

  return analysisText;
}

// ä¸ºå•å¸§ç”Ÿæˆæç¤ºè¯ï¼ˆåŸºäºä¸Šä¸€å¸§ï¼‰
async function generateFramePrompt(
  anthropic: Anthropic,
  baseAnalysis: string,
  animationPrompt: string,
  frameIndex: number,
  previousFrameUrl: string | null,
  isFirstFrame: boolean
): Promise<string> {
  const framePosition = frameIndex + 1;
  const animationPhase = 
    frameIndex < 3 ? "building up" :
    frameIndex < 6 ? "peak intensity" :
    frameIndex < 9 ? "winding down" : "returning to start";

  const prompt = isFirstFrame
    ? `Based on this image analysis, generate a detailed image prompt for frame 1 of a 10-frame "${animationPrompt}" animation.

Analysis: ${baseAnalysis}

This is the STARTING frame. The subject should be in its initial/neutral state, ready to begin the animation.

Output ONLY the image generation prompt in English (100+ words), no explanations.`
    : `Generate the image prompt for frame ${framePosition}/10 of a "${animationPrompt}" animation.

Base analysis: ${baseAnalysis}

Animation phase: ${animationPhase}
Previous frame was frame ${frameIndex}.

CRITICAL RULES:
- Frame ${framePosition} must be only 5-10% different from frame ${frameIndex}
- The change must be TINY and gradual
- Maintain EXACT same: subject appearance, background, art style, colors
- Only change the specific animation element (${animationPrompt})
${frameIndex === 9 ? '- This is the LAST frame - must transition smoothly back to frame 1' : ''}

Output ONLY the image generation prompt in English (100+ words), no explanations.`;

  const response = await anthropic.messages.create({
    model: "claude-sonnet-4-5-20250929",
    max_tokens: 500,
    messages: [{ role: "user", content: prompt }],
  });

  const textBlock = response.content.find(b => b.type === "text");
  return textBlock?.type === "text" ? textBlock.text : `Frame ${framePosition} of ${animationPrompt} animation`;
}

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: any) => {
    await writer.write(encoder.encode(`data: ${JSON.stringify(event)}\n\n`));
  };

  (async () => {
    try {
      const body = await request.json();
      const { referenceImage, animationPrompt, model, config } = body;

      if (!referenceImage) {
        await sendEvent({ type: "error", error: "ç¼ºå°‘å‚è€ƒå›¾ç‰‡" });
        await writer.close();
        return;
      }

      if (!animationPrompt) {
        await sendEvent({ type: "error", error: "ç¼ºå°‘åŠ¨ç”»æè¿°" });
        await writer.close();
        return;
      }

      const taskId = uuidv4();

      // Step 1: åˆ†æåŸå§‹å›¾ç‰‡ï¼ˆæµå¼æ˜¾ç¤ºï¼‰
      await sendEvent({
        type: "status",
        step: "ğŸ‘ï¸ Claude æ­£åœ¨åˆ†æå‚è€ƒå›¾ç‰‡...",
        progress: 5,
      });
      await sendEvent({ type: "claude_analysis_start" });

      let baseAnalysis = "";
      try {
        baseAnalysis = await analyzeOriginalImage(
          referenceImage,
          animationPrompt,
          async (chunk) => {
            await sendEvent({ type: "claude_analysis_chunk", chunk });
          }
        );
      } catch (err) {
        console.error("Analysis error:", err);
        await sendEvent({ type: "error", error: "å›¾ç‰‡åˆ†æå¤±è´¥" });
        await writer.close();
        return;
      }

      await sendEvent({ type: "claude_analysis_end" });

      // Step 2: åˆ›å»ºä»»åŠ¡è®°å½•
      await sendEvent({
        type: "status",
        step: "ğŸ“ åˆ›å»ºä»»åŠ¡...",
        progress: 15,
      });

      await prisma.stickerTask.create({
        data: {
          id: taskId,
          status: "processing",
          animationType: animationPrompt,
          referenceImage,
          model: model || "nano-banana",
          config: JSON.stringify(config || {}),
          customPrompt: baseAnalysis, // å­˜å‚¨åŸºç¡€åˆ†æ
          totalFrames: 10,
          completedFrames: 0,
          frames: JSON.stringify([]),
          frameStatuses: JSON.stringify(Array(10).fill("pending")),
        },
      });

      // Step 3: ç«‹å³åˆ›å»º StickerNode
      await sendEvent({
        type: "sticker_created",
        taskId,
      });

      await sendEvent({
        type: "status",
        step: "ğŸš€ ä»»åŠ¡å·²åˆ›å»ºï¼Œå¼€å§‹é“¾å¼ç”Ÿæˆ...",
        progress: 20,
      });

      // Step 4: åå°é“¾å¼ç”Ÿæˆï¼ˆä¸ç­‰å¾…ï¼‰
      processChainedFrames(
        taskId,
        baseAnalysis,
        animationPrompt,
        referenceImage,
        (model || "nano-banana") as GeminiImageModel,
        config || {}
      ).catch((err) => {
        console.error(`[Sticker ${taskId}] Chain generation error:`, err);
      });

      await sendEvent({
        type: "complete",
        progress: 100,
      });

      await writer.close();
    } catch (error) {
      console.error("Sticker generation error:", error);
      await sendEvent({
        type: "error",
        error: error instanceof Error ? error.message : "ç”Ÿæˆå¤±è´¥",
      });
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

// åå°é“¾å¼ç”Ÿæˆï¼šæ¯å¸§åŸºäºä¸Šä¸€å¸§
async function processChainedFrames(
  taskId: string,
  baseAnalysis: string,
  animationPrompt: string,
  originalReferenceImage: string,
  model: GeminiImageModel,
  config: ImageGenerationConfig
) {
  const anthropic = getAnthropicClient();
  const generatedFrames: (string | null)[] = Array(10).fill(null);
  const frameStatuses: string[] = Array(10).fill("pending");

  console.log(`[Sticker ${taskId}] Starting chained generation...`);

  // å½“å‰å‚è€ƒå›¾ï¼šåˆå§‹ä¸ºåŸå§‹å›¾ï¼Œä¹‹åç”¨ä¸Šä¸€å¸§
  let currentReferenceImage = originalReferenceImage;

  for (let i = 0; i < 10; i++) {
    frameStatuses[i] = "generating";
    
    await prisma.stickerTask.update({
      where: { id: taskId },
      data: { frameStatuses: JSON.stringify(frameStatuses) },
    });

    try {
      // Step A: ä¸ºå½“å‰å¸§ç”Ÿæˆæç¤ºè¯
      console.log(`[Sticker ${taskId}] Generating prompt for frame ${i + 1}...`);
      const framePrompt = await generateFramePrompt(
        anthropic,
        baseAnalysis,
        animationPrompt,
        i,
        i > 0 ? generatedFrames[i - 1] : null,
        i === 0
      );
      console.log(`[Sticker ${taskId}] Frame ${i + 1} prompt: ${framePrompt.substring(0, 80)}...`);

      // Step B: ç”¨ä¸Šä¸€å¸§ä½œä¸ºå‚è€ƒç”Ÿæˆå½“å‰å¸§å›¾ç‰‡
      console.log(`[Sticker ${taskId}] Generating image for frame ${i + 1}...`);
      const result = await generateImageAction(
        framePrompt,
        model,
        { ...config, aspectRatio: "1:1" },
        [currentReferenceImage]
      );

      if (result.success && result.imageUrl) {
        generatedFrames[i] = result.imageUrl;
        frameStatuses[i] = "completed";
        // æ›´æ–°å‚è€ƒå›¾ä¸ºå½“å‰å¸§
        currentReferenceImage = result.imageUrl;
        console.log(`[Sticker ${taskId}] Frame ${i + 1} completed âœ“`);
      } else {
        frameStatuses[i] = "error";
        console.error(`[Sticker ${taskId}] Frame ${i + 1} failed:`, result.error);
      }
    } catch (err) {
      frameStatuses[i] = "error";
      console.error(`[Sticker ${taskId}] Frame ${i + 1} error:`, err);
    }

    // æ›´æ–°æ•°æ®åº“
    const completedFrameUrls = generatedFrames.filter((f): f is string => f !== null);
    await prisma.stickerTask.update({
      where: { id: taskId },
      data: {
        frames: JSON.stringify(completedFrameUrls),
        frameStatuses: JSON.stringify(frameStatuses),
        completedFrames: frameStatuses.filter(s => s === "completed").length,
      },
    });
  }

  // æœ€ç»ˆçŠ¶æ€
  const completedCount = frameStatuses.filter(s => s === "completed").length;
  await prisma.stickerTask.update({
    where: { id: taskId },
    data: {
      status: completedCount >= 5 ? "completed" : "failed",
      frames: JSON.stringify(generatedFrames.filter(f => f !== null)),
      frameStatuses: JSON.stringify(frameStatuses),
      completedFrames: completedCount,
      completedAt: new Date(),
    },
  });

  console.log(`[Sticker ${taskId}] âœ… Chained generation finished (${completedCount}/10 frames)`);
}
