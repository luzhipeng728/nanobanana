import { NextRequest } from "next/server";
import Anthropic from "@anthropic-ai/sdk";

export interface VeoAnalyzeEvent {
  type: "status" | "analysis_start" | "analysis_chunk" | "analysis_end" | "prompt_ready" | "error";
  step?: string;
  progress?: number;
  chunk?: string;
  prompt?: string;
  analysis?: string;
  error?: string;
}

/**
 * æµå¼åˆ†æå›¾ç‰‡å¹¶ç”Ÿæˆè§†é¢‘æç¤ºè¯
 * ç±»ä¼¼ Agent çš„ä¸¤é˜¶æ®µæµç¨‹ï¼šå…ˆåˆ†æå›¾ç‰‡ï¼Œå†ç”Ÿæˆæç¤ºè¯
 */
export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();

  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: VeoAnalyzeEvent) => {
    await writer.write(encoder.encode(`data: ${JSON.stringify(event)}\n\n`));
  };

  (async () => {
    try {
      const body = await request.json();
      const { userRequest, imageUrl } = body as {
        userRequest: string;
        imageUrl?: string;
      };

      if (!userRequest) {
        await sendEvent({ type: "error", error: "è¯·è¾“å…¥è§†é¢‘æè¿°" });
        await writer.close();
        return;
      }

      const apiKey = process.env.ANTHROPIC_API_KEY;
      if (!apiKey) {
        await sendEvent({ type: "error", error: "ANTHROPIC_API_KEY æœªé…ç½®" });
        await writer.close();
        return;
      }

      const anthropic = new Anthropic({
        apiKey,
        baseURL: process.env.ANTHROPIC_BASE_URL || undefined,
      });

      let imageAnalysis = "";

      // ç¬¬ä¸€é˜¶æ®µï¼šå¦‚æœæœ‰å›¾ç‰‡ï¼Œå…ˆè¯¦ç»†åˆ†æå›¾ç‰‡
      if (imageUrl) {
        await sendEvent({
          type: "status",
          step: "ğŸ‘ï¸ Claude æ­£åœ¨åˆ†æå‚è€ƒå›¾ç‰‡...",
          progress: 10,
        });

        await sendEvent({ type: "analysis_start" });

        // æ„å»ºå›¾ç‰‡å†…å®¹
        const imageContent: Anthropic.ImageBlockParam[] = [];
        if (imageUrl.startsWith("data:")) {
          const match = imageUrl.match(/^data:([^;]+);base64,(.+)$/);
          if (match) {
            imageContent.push({
              type: "image",
              source: {
                type: "base64",
                media_type: match[1] as "image/jpeg" | "image/png" | "image/gif" | "image/webp",
                data: match[2],
              },
            });
          }
        } else {
          imageContent.push({
            type: "image",
            source: {
              type: "url",
              url: imageUrl,
            },
          });
        }

        // æµå¼åˆ†æå›¾ç‰‡
        const analysisStream = anthropic.messages.stream({
          model: "claude-sonnet-4-20250514",
          max_tokens: 1024,
          messages: [
            {
              role: "user",
              content: [
                ...imageContent,
                {
                  type: "text",
                  text: `è¯·ä»”ç»†åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œä¸ºè§†é¢‘ç”Ÿæˆåšå‡†å¤‡ã€‚

ç”¨æˆ·æƒ³è¦çš„æ•ˆæœï¼š${userRequest}

è¯·è¯¦ç»†æè¿°ï¼š
1. **ä¸»ä½“åˆ†æ**ï¼šå›¾ç‰‡ä¸­çš„äººç‰©/ç‰©ä½“çš„å§¿æ€ã€è¡¨æƒ…ã€æœè£…ã€ç‰¹å¾
2. **åœºæ™¯ç¯å¢ƒ**ï¼šèƒŒæ™¯ã€å…‰çº¿ã€è‰²è°ƒã€æ°›å›´
3. **æ„å›¾é£æ ¼**ï¼šæ‹æ‘„è§’åº¦ã€æ™¯æ·±ã€è‰ºæœ¯é£æ ¼
4. **åŠ¨æ€å»ºè®®**ï¼šåŸºäºç”¨æˆ·éœ€æ±‚ï¼Œå»ºè®®å“ªäº›åŠ¨ä½œ/è¿é•œæœ€åˆé€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæè¿°è¦è¯¦ç»†å…·ä½“ã€‚`,
                },
              ],
            },
          ],
        });

        for await (const event of analysisStream) {
          if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
            const chunk = event.delta.text;
            imageAnalysis += chunk;
            await sendEvent({ type: "analysis_chunk", chunk });
          }
        }

        await sendEvent({ type: "analysis_end" });

        await sendEvent({
          type: "status",
          step: "âœ… å›¾ç‰‡åˆ†æå®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆè§†é¢‘æç¤ºè¯...",
          progress: 50,
        });
      } else {
        await sendEvent({
          type: "status",
          step: "ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘æç¤ºè¯...",
          progress: 30,
        });
      }

      // ç¬¬äºŒé˜¶æ®µï¼šåŸºäºåˆ†æç»“æœç”Ÿæˆè§†é¢‘æç¤ºè¯
      const promptSystemMessage = imageUrl
        ? `You are a professional video prompt engineer for Google Veo 3.1 (image-to-video mode).

Based on the image analysis below, generate a video prompt that animates this image.

## Image Analysis:
${imageAnalysis}

## User's Request: ${userRequest}

## Veo 3.1 Image-to-Video Best Practices:

**CRITICAL RULES:**
1. The source image already provides background and style - ONLY describe the motion/animation
2. Use generic terms like "the subject", "the woman", "the figure" - DO NOT re-describe physical features
3. Focus on THREE types of motion:
   - Camera movement (pan, tilt, zoom, tracking, crane, dolly)
   - Subject animation (walking, turning, gesturing, expressions)
   - Environment animation (wind, water, particles, lighting changes)

**Prompt Structure:**
[Camera Movement] + [Subject Action] + [Environment Animation] + [Mood/Atmosphere]

**AVOID:**
- Re-describing what's already in the image
- Complex multi-event narratives
- Using quotes for dialogue

**Examples:**
- "Slow dolly in, the subject turns her head and smiles softly, hair gently swaying in the breeze, warm afternoon light"
- "Camera slowly pans right, the figure walks forward confidently, leaves rustling in the background"

Generate a concise prompt (30-60 words) focusing ONLY on the animation/motion.
Output ONLY the prompt text in English, nothing else.`
        : `You are a professional video prompt engineer for Google Veo 3.1 (text-to-video mode).

User's request: ${userRequest}

## Veo 3.1 Text-to-Video Best Practices:

**Include these elements:**
1. **Subject**: Who/what is the focus
2. **Action**: What's happening
3. **Scene/Setting**: Where and when
4. **Camera**: Angle and movement
5. **Style**: Visual aesthetic
6. **Lighting**: Light sources and mood
7. **Atmosphere**: Environmental effects

**Example:**
"Cinematic close-up of a wise elderly woman, weathered hands holding a glowing crystal, warm candlelight casting soft shadows, slow push-in, mystical atmosphere"

Generate a detailed prompt (50-80 words) with cinematic depth.
Output ONLY the prompt text in English, nothing else.`;

      const promptResponse = await anthropic.messages.create({
        model: "claude-sonnet-4-20250514",
        max_tokens: 500,
        messages: [{ role: "user", content: promptSystemMessage }],
      });

      const textBlock = promptResponse.content.find(
        (block): block is Anthropic.TextBlock => block.type === "text"
      );

      const generatedPrompt = textBlock?.text?.trim() || userRequest;

      await sendEvent({
        type: "status",
        step: "âœ¨ è§†é¢‘æç¤ºè¯ç”Ÿæˆå®Œæˆï¼",
        progress: 100,
      });

      await sendEvent({
        type: "prompt_ready",
        prompt: generatedPrompt,
        analysis: imageAnalysis || undefined,
      });
    } catch (error) {
      console.error("Veo analyze error:", error);
      await sendEvent({
        type: "error",
        error: error instanceof Error ? error.message : "åˆ†æå¤±è´¥",
      });
    } finally {
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
